diff --git a/videollava/model/builder.py b/videollava/model/builder.py
index ac6ac95..b66f06e 100644
--- a/videollava/model/builder.py
+++ b/videollava/model/builder.py
@@ -41,7 +41,7 @@ def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, l
             bnb_4bit_quant_type='nf4'
         )
     else:
-        kwargs['torch_dtype'] = torch.float16
+        kwargs['torch_dtype'] = torch.bfloat16
 
     if 'llava' in model_name.lower():
         # Load LLaVA model
@@ -145,7 +145,7 @@ def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, l
             image_tower = model.get_image_tower()
             if not image_tower.is_loaded:
                 image_tower.load_model()
-            image_tower.to(device=device, dtype=torch.float16)
+            image_tower.to(device=device, dtype=torch.bfloat16)
             image_processor = image_tower.image_processor
             processor['image'] = image_processor
 
@@ -153,7 +153,7 @@ def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, l
             video_tower = model.get_video_tower()
             if not video_tower.is_loaded:
                 video_tower.load_model()
-            video_tower.to(device=device, dtype=torch.float16)
+            video_tower.to(device=device, dtype=torch.bfloat16)
             video_processor = video_tower.video_processor
             processor['video'] = video_processor
     # ==========================================================================================================
@@ -162,5 +162,4 @@ def load_pretrained_model(model_path, model_base, model_name, load_8bit=False, l
         context_len = model.config.max_sequence_length
     else:
         context_len = 2048
-
     return tokenizer, model, processor, context_len
diff --git a/videollava/serve/cli.py b/videollava/serve/cli.py
index 95aa6d7..3626a4b 100644
--- a/videollava/serve/cli.py
+++ b/videollava/serve/cli.py
@@ -17,7 +17,7 @@ import requests
 from PIL import Image
 from io import BytesIO
 from transformers import TextStreamer
-
+import time
 
 
 
@@ -30,6 +30,13 @@ def main(args):
     tokenizer, model, processor, context_len = load_pretrained_model(args.model_path, args.model_base, model_name,
                                                                      args.load_8bit, args.load_4bit,
                                                                      device=args.device, cache_dir=args.cache_dir)
+
+
+    model.eval()
+    if args.mode=='ipex':  
+      import intel_extension_for_pytorch as ipex
+      model = ipex.optimize(model, dtype=torch.bfloat16)
+
     image_processor, video_processor = processor['image'], processor['video']
     if 'llama-2' in model_name.lower():
         conv_mode = "llava_llama_2"
@@ -54,12 +61,13 @@ def main(args):
     tensor = []
     special_token = []
     args.file = args.file if isinstance(args.file, list) else [args.file]
+
     for file in args.file:
         if os.path.splitext(file)[-1].lower() in image_ext:
-            file = image_processor.preprocess(file, return_tensors='pt')['pixel_values'][0].to(model.device, dtype=torch.float16)
+            file = image_processor.preprocess(file, return_tensors='pt')['pixel_values'][0].to(model.device, dtype=torch.bfloat16)
             special_token += [DEFAULT_IMAGE_TOKEN]
         elif os.path.splitext(file)[-1].lower() in video_ext:
-            file = video_processor(file, return_tensors='pt')['pixel_values'][0].to(model.device, dtype=torch.float16)
+            file = video_processor(file, return_tensors='pt')['pixel_values'][0].to(model.device, dtype=torch.bfloat16)
             special_token += [DEFAULT_IMAGE_TOKEN] * model.get_video_tower().config.num_frames
         else:
             raise ValueError(f'Support video of {video_ext} and image of {image_ext}, but found {os.path.splitext(file)[-1].lower()}')
@@ -93,14 +101,16 @@ def main(args):
             conv.append_message(conv.roles[0], inp)
         conv.append_message(conv.roles[1], None)
         prompt = conv.get_prompt()
-
+        no_of_tokens=[]
+        latency=[]
         input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)
         stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2
         keywords = [stop_str]
         stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)
         streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
 
-        with torch.inference_mode():
+        start_time = time.time()
+        with torch.inference_mode(),torch.cpu.amp.autocast():
             output_ids = model.generate(
                 input_ids,
                 images=tensor,  # video as fake images
@@ -110,12 +120,17 @@ def main(args):
                 streamer=streamer,
                 use_cache=True,
                 stopping_criteria=[stopping_criteria])
-
+        end_time = time.time()
+        latency.append(end_time - start_time)
+        no_of_tokens.append(len(tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()))
+        
         outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()
         conv.messages[-1][-1] = outputs
 
         if args.debug:
             print("\n", {"prompt": prompt, "outputs": outputs}, "\n")
+            
+    print("average token latency : ", sum(latency)/sum(no_of_tokens))
 
 
 if __name__ == "__main__":
@@ -130,6 +145,7 @@ if __name__ == "__main__":
     parser.add_argument("--max-new-tokens", type=int, default=512)
     parser.add_argument("--load-8bit", action="store_true")
     parser.add_argument("--load-4bit", action="store_true")
+    parser.add_argument("--mode", type=str, default="pt", help="pt or ipex")
     parser.add_argument("--debug", action="store_true")
     args = parser.parse_args()
     main(args)
